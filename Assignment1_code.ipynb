{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open(\"datav2.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df = df[(df['works_count'] > 5) & (df['works_count'] < 5000)]  # Filtering\n",
    "\n",
    "papers = pd.DataFrame(columns=['id', 'publication_year', 'cited_by_count', 'author_ids'])\n",
    "abstracts = pd.DataFrame(columns=['id', 'title', 'abstract_inverted_index'])\n",
    "\n",
    "# Define concept IDs\n",
    "concept_ids = [\n",
    "    \"C144024400\",  # Sociology\n",
    "    \"C15744967\",   # Psychology\n",
    "    \"C162324750\",  # Economics\n",
    "    \"C17744445\",   # Political Science\n",
    "    \"C33923547\",   # Mathematics\n",
    "    \"C121332964\",  # Physics\n",
    "    \"C41008148\",   # Computer Science\n",
    "]\n",
    "\n",
    "# Initialize DataFrames\n",
    "paperdata = []\n",
    "abstractdata = []\n",
    "\n",
    "def get_data(i):\n",
    "    ids = [aut.split(\"id:\")[1] for aut in i]\n",
    "    BASE_URL = (\n",
    "        f\"https://api.openalex.org/works?filter=author.id:({(\"|\").join(ids)}),cited_by_count:>10,\"\n",
    "        f\"authors_count:<10,concept.id:({'|'.join(concept_ids[:4])}),concept.id:({'|'.join(concept_ids[4:])})\"\n",
    "    )\n",
    "\n",
    "    retries = 0\n",
    "    papers = []\n",
    "    abstracts = []\n",
    "\n",
    "    while retries < 3:\n",
    "        try:\n",
    "            response = requests.get(BASE_URL + \"&per-page=200&cursor=*\").json()\n",
    "\n",
    "            while response.get(\"results\"):\n",
    "                for result in response[\"results\"]:\n",
    "                    papers.append({\n",
    "                        \"id\": result.get(\"id\"),\n",
    "                        \"publication_year\": result.get(\"publication_year\"),\n",
    "                        \"cited_by_count\": result.get(\"cited_by_count\"),\n",
    "                        \"author_ids\": [\n",
    "                            auth[\"author\"][\"id\"]\n",
    "                            for auth in result.get(\"authorships\", [])\n",
    "                            if \"author\" in auth and \"id\" in auth[\"author\"]\n",
    "                        ],\n",
    "                    })\n",
    "                    abstracts.append({\n",
    "                        \"id\": result.get(\"id\"),\n",
    "                        \"title\": result.get(\"title\"),\n",
    "                        \"abstract_inverted_index\": result.get(\"abstract_inverted_index\"),\n",
    "                    })\n",
    "\n",
    "                next_cursor = response.get(\"meta\", {}).get(\"next_cursor\")\n",
    "                if not next_cursor:\n",
    "                    break\n",
    "\n",
    "                time.sleep(1) \n",
    "                response = requests.get(BASE_URL + f\"&per-page=200&cursor={next_cursor}\").json()\n",
    "\n",
    "            return papers, abstracts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching work ID {ids}: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    return [], []\n",
    "\n",
    "# Parallel processing\n",
    "num_batch = 5\n",
    "batch_size = 100 \n",
    "\n",
    "for i in tqdm(range(0, len(df[\"works_api_url\"]), batch_size)):\n",
    "    batch_indexes = df[\"works_api_url\"][i:i+100].tolist()\n",
    "    batches = [batch_indexes[i:i+25] for i in range(0,100,25)]\n",
    "    # Fetch data in parallel\n",
    "    results = Parallel(n_jobs=num_batch)(\n",
    "        delayed(get_data)(batch) for batch in batches\n",
    "    )\n",
    "\n",
    "    # Collect results\n",
    "    for pap, abs in results:\n",
    "        if pap and abs:\n",
    "            paperdata.extend(pap)  # Use list extend for efficiency\n",
    "            abstractdata.extend(abs)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "# Convert collected lists into DataFrames\n",
    "paperdata_df = pd.DataFrame(paperdata)\n",
    "abstractdata_df = pd.DataFrame(abstractdata)\n",
    "\n",
    "# Save results\n",
    "paperdata_df.to_csv(\"papers.csv\", index=False)\n",
    "abstractdata_df.to_csv(\"abstracts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
