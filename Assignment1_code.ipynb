{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:36<00:00,  8.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "with open(\"datav2.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df = df[(df['works_count'] > 5) & (df['works_count'] < 5000)]  # Filtering\n",
    "\n",
    "#Initialize DataFrames\n",
    "papers = pd.DataFrame(columns=['id', 'publication_year', 'cited_by_count', 'author_ids'])\n",
    "abstracts = pd.DataFrame(columns=['id', 'title', 'abstract_inverted_index'])\n",
    "\n",
    "# Define concept IDs\n",
    "concept_ids = [\n",
    "    \"C144024400\",  # Sociology\n",
    "    \"C15744967\",   # Psychology\n",
    "    \"C162324750\",  # Economics\n",
    "    \"C17744445\",   # Political Science\n",
    "    \"C33923547\",   # Mathematics\n",
    "    \"C121332964\",  # Physics\n",
    "    \"C41008148\",   # Computer Science\n",
    "]\n",
    "\n",
    "paperdata = []\n",
    "abstractdata = []\n",
    "\n",
    "def get_data(i):\n",
    "    ids = [aut.split(\"id:\")[1] for aut in i]\n",
    "    BASE_URL = (\n",
    "        f\"https://api.openalex.org/works?filter=author.id:({(\"|\").join(ids)}),cited_by_count:>10,\"\n",
    "        f\"authors_count:<10,concept.id:({'|'.join(concept_ids[:4])}),concept.id:({'|'.join(concept_ids[4:])})\"\n",
    "    )\n",
    "\n",
    "    retries = 0\n",
    "    papers = []\n",
    "    abstracts = []\n",
    "\n",
    "    while retries < 3:\n",
    "        try:\n",
    "            response = requests.get(BASE_URL + \"&per-page=200&cursor=*\").json()\n",
    "\n",
    "            while response.get(\"results\"):\n",
    "                for result in response[\"results\"]:\n",
    "                    papers.append({\n",
    "                        \"id\": result.get(\"id\"),\n",
    "                        \"publication_year\": result.get(\"publication_year\"),\n",
    "                        \"cited_by_count\": result.get(\"cited_by_count\"),\n",
    "                        \"author_ids\": [\n",
    "                            auth[\"author\"][\"id\"]\n",
    "                            for auth in result.get(\"authorships\", [])\n",
    "                            if \"author\" in auth and \"id\" in auth[\"author\"]\n",
    "                        ],\n",
    "                    })\n",
    "                    abstracts.append({\n",
    "                        \"id\": result.get(\"id\"),\n",
    "                        \"title\": result.get(\"title\"),\n",
    "                        \"abstract_inverted_index\": result.get(\"abstract_inverted_index\"),\n",
    "                    })\n",
    "\n",
    "                next_cursor = response.get(\"meta\", {}).get(\"next_cursor\")\n",
    "                if not next_cursor:\n",
    "                    break\n",
    "\n",
    "                time.sleep(1) \n",
    "                response = requests.get(BASE_URL + f\"&per-page=200&cursor={next_cursor}\").json()\n",
    "\n",
    "            return papers, abstracts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching work ID {ids}: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    return [], []\n",
    "\n",
    "# Parallel processing\n",
    "num_batch = 5\n",
    "batch_size = 100 \n",
    "\n",
    "for i in tqdm(range(0, len(df[\"works_api_url\"]), batch_size)):\n",
    "    batch_indexes = df[\"works_api_url\"][i:i+100].tolist()\n",
    "    batches = [batch_indexes[i:i+25] for i in range(0,100,25)]\n",
    "    #Retrieve data in parallel\n",
    "    results = Parallel(n_jobs=num_batch)(\n",
    "        delayed(get_data)(batch) for batch in batches\n",
    "    )\n",
    "\n",
    "    # Collect results\n",
    "    for pap, abs in results:\n",
    "        if pap and abs:\n",
    "            paperdata.extend(pap) \n",
    "            abstractdata.extend(abs)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "#Convert collected lists into DataFrames\n",
    "paperdata_df = pd.DataFrame(paperdata)\n",
    "abstractdata_df = pd.DataFrame(abstractdata)\n",
    "\n",
    "# Save results\n",
    "paperdata_df.to_csv(\"papers.csv\", index=False)\n",
    "abstractdata_df.to_csv(\"abstracts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13362"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many works are listed? (id in this dataframe is an id of a work)\n",
    "len(paperdata_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18004"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many unique reasearchers have co-authored these works?\n",
    "(pd.DataFrame(paperdata_df.explode('author_ids'))['author_ids']).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **Dataset summary.**\n",
    "\n",
    "There is a total of 13362 works listed in the *IC2S2 papers* dataframe with 18004 unique researchers who have co-authored these works.\n",
    "> - **Efficiency in code.** Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time? __(answer in max 150 words)__\n",
    "\n",
    "When retrieving data from the API, the request would often have trouble fetching some of the data, resulting in a very long run-time. To address this, we implemented a try-except block, which retries the request a set number of tries, before moving on to the next request. In addition, we also implemented multiprocessing to make multiple requests at the same time, with the use of Parallel and tqdm - this significantly reduced the run-time of the code. Lastly applying filters in the API request, ensured that only relevant data was taken into consideration. All of these strategies combined, improved the execution time of the code, from taking close to an hour to run, to only about a minute or two.\n",
    "\n",
    "> - **Filtering Criteria and Dataset Relevance** Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? __(answer in max 150 words)__\n",
    "\n",
    "Using specific thresholds ensures that the fetched data is both relevant and of high quality, but makes data collecting more manageable, as fewer works need to be processed. While the threshold for total number of works by an author makes sure that very prolific authors are not overrepresented, this however also excludes inactive or emerging authors. Similarly, the citation threshold highlights influential and relevant studies, but may overlook newer works that have yet to gain recognition. Limiting the number of authors per work favors small collaborations over large, potentially filtering out large interdisciplinary research. Field-based filtering broadens the scope to include relevant interdisciplinary studies but may still underrepresent qualitative approaches. Applying filters enhances the relevance of the dataset, but may also introduce bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
